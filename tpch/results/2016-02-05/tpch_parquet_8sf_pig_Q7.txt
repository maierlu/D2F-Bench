WARNING: Use "yarn jar" to launch YARN applications.
16/02/05 07:23:44 INFO pig.ExecTypeProvider: Trying ExecType : LOCAL
16/02/05 07:23:44 INFO pig.ExecTypeProvider: Trying ExecType : MAPREDUCE
16/02/05 07:23:44 INFO pig.ExecTypeProvider: Picked MAPREDUCE as the ExecType
2016-02-05 07:23:44,493 [main] INFO  org.apache.pig.Main - Apache Pig version 0.15.0.2.3.2.0-2950 (rexported) compiled Sep 30 2015, 19:39:20
2016-02-05 07:23:44,493 [main] INFO  org.apache.pig.Main - Logging error messages to: /root/D2F-Bench/bin/pig_1454657024490.log
2016-02-05 07:23:45,537 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found
2016-02-05 07:23:45,807 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://sandbox.hortonworks.com:8020
2016-02-05 07:23:47,562 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
2016-02-05 07:23:48,529 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 8
2016-02-05 07:23:48,925 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 8
2016-02-05 07:23:49,061 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 8
2016-02-05 07:23:49,243 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1
2016-02-05 07:23:49,311 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1
2016-02-05 07:23:49,562 [main] WARN  org.apache.pig.newplan.BaseOperatorPlan - Encountered Warning IMPLICIT_CAST_TO_DOUBLE 1 time(s).
2016-02-05 07:23:49,664 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: REPLICATED_JOIN,HASH_JOIN,GROUP_BY,ORDER_BY,FILTER
2016-02-05 07:23:49,718 [main] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.
2016-02-05 07:23:49,777 [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2016-02-05 07:23:49,958 [main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned for orders: $2, $3, $4, $5, $6, $7, $8
2016-02-05 07:23:49,961 [main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned for customer: $1, $2, $4, $5, $6, $7
2016-02-05 07:23:49,961 [main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned for supplier: $1, $2, $4, $5, $6
2016-02-05 07:23:49,962 [main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned for lineitem0: $1, $3, $4, $7, $8, $9, $11, $12, $13, $14, $15
2016-02-05 07:23:49,963 [main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned for nation20: $2, $3
2016-02-05 07:23:49,970 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2000: Error processing rule ColumnMapKeyPrune. Try -t ColumnMapKeyPrune
Details at logfile: /root/D2F-Bench/bin/pig_1454657024490.log
2016-02-05 07:23:49,995 [main] INFO  org.apache.pig.Main - Pig script completed in 5 seconds and 759 milliseconds (5759 ms)
Feb 5, 2016 7:23:47 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 1
Feb 5, 2016 7:23:47 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Feb 5, 2016 7:23:47 AM INFO: parquet.hadoop.ParquetFileReader: reading another 1 footers
Feb 5, 2016 7:23:47 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Feb 5, 2016 7:23:48 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 8
Feb 5, 2016 7:23:48 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Feb 5, 2016 7:23:48 AM INFO: parquet.hadoop.ParquetFileReader: reading another 8 footers
Feb 5, 2016 7:23:48 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Feb 5, 2016 7:23:48 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 8
Feb 5, 2016 7:23:48 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Feb 5, 2016 7:23:48 AM INFO: parquet.hadoop.ParquetFileReader: reading another 8 footers
Feb 5, 2016 7:23:48 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Feb 5, 2016 7:23:49 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 8
Feb 5, 2016 7:23:49 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Feb 5, 2016 7:23:49 AM INFO: parquet.hadoop.ParquetFileReader: reading another 8 footers
Feb 5, 2016 7:23:49 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Feb 5, 2016 7:23:49 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 1
Feb 5, 2016 7:23:49 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Feb 5, 2016 7:23:49 AM INFO: parquet.hadoop.ParquetFileReader: reading another 1 footers
Feb 5, 2016 7:23:49 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Feb 5, 2016 7:23:49 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 1
